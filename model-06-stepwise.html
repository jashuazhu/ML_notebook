<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>6. Stepwise Regression — Regression Models Handbook</title>
<link rel="stylesheet" href="styles.css" />

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['\\[', '\\]'], ['$$','$$']],
    processEscapes: true,
    tags: 'ams'
  },
  options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
};
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</head>
<body>
<main class="book-page">
  <header class="book-header">
    <p class="lead">Regression Models Handbook</p>
    <h1>6. Stepwise Regression</h1>
  </header>

  
    <nav class="chapter-nav top">
      <a href="model-05-ridge.html">&larr; Prev</a>
      <a class="index-link" href="index.html">Index</a>
      <a href="model-07-logistic.html">Next &rarr;</a>
    </nav>
    

  <section class="card">
    <h2>Overview & Background</h2>
    <p>Stepwise (forward/backward) iteratively adds/removes variables using criteria (AIC/BIC) to select a subset model. Fast and pragmatic, but can be unstable and overfit without care.</p>
  </section>

  <section class="card">
    <h2>Mathematics & Derivation</h2>
    <p class="formula">At each step, choose the move that best improves an information criterion, e.g. $\mathrm{AIC}=2k-2\log\hat{L}$ or $\mathrm{BIC}=k\log n - 2\log\hat{L}$, where $k$ is the number of parameters and $\hat{L}$ the maximized likelihood.</p>
    <p class="muted"></p>
  </section>

  <section class="card">
    <h2>When to Use (Best For)</h2>
    <ul>
      <li>Quick variable screening from many candidates</li>
<li>Baseline model before regularization methods</li>
<li>When interpretability and parsimony matter</li>
    </ul>
  </section>

  <section class="card">
    <h2>Example & Code</h2>
    <p>Below we simulate data with white noise, fit the model, and create a plot showing original data, fitted values, and residuals. Run this locally (see comments for required packages).</p>
    <details>
      <summary>Show Python example</summary>
      <pre class="ex"><code># Requirements: numpy, matplotlib, scikit-learn (or statsmodels/lifelines as noted).
# This example simulates data with white noise, fits the model, and plots:
#   - original data
#   - fitted curve/predictions
#   - residuals (errors)
import numpy as np
import matplotlib.pyplot as plt
rng = np.random.default_rng(42)

# pip install statsmodels
import numpy as np
import statsmodels.api as sm

def forward_stepwise(X, y, feature_names, criterion=&#x27;aic&#x27;):
    selected = []
    remaining = list(range(X.shape[1]))
    current_score = np.inf
    while remaining:
        scores = []
        for j in remaining:
            cand = selected + [j]
            Xc = sm.add_constant(X[:, cand])
            model = sm.OLS(y, Xc).fit()
            score = model.aic if criterion==&#x27;aic&#x27; else model.bic
            scores.append((score, j, model))
        scores.sort(key=lambda t: t[0])
        best_score, best_j, best_model = scores[0]
        if best_score &lt; current_score - 1e-6:
            current_score = best_score
            selected.append(best_j)
            remaining.remove(best_j)
        else:
            break
    return selected, best_model

rng = np.random.default_rng(0)
n, p = 250, 12
X = rng.normal(size=(n, p))
beta = np.zeros(p); beta[[1,4,7]] = [2.0, -1.5, 0.9]
y = 1.0 + X @ beta + rng.normal(scale=1.2, size=n)

names = [f&#x27;x{j}&#x27; for j in range(p)]
sel, model = forward_stepwise(X, y, names, criterion=&#x27;aic&#x27;)

x = X[:, [sel[0]]] if sel else X[:, [0]]
x_grid = np.linspace(x.min(), x.max(), 200).reshape(-1,1)
# Build grid for plotting varying first selected feature only
Xg = np.zeros((len(x_grid), X.shape[1]))
Xg[:, sel[0] if sel else 0] = x_grid[:,0]
y_pred_grid = model.predict(sm.add_constant(Xg))

y_pred = model.predict(sm.add_constant(X))
residuals = y - y_pred

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

axes[0].scatter(x, y, s=12, label=&#x27;Original data&#x27;)
axes[0].plot(x_grid, y_pred_grid, linewidth=2, label=&#x27;Fitted&#x27;)
axes[0].set_title(&#x27;Fit&#x27;)
axes[0].legend()

axes[1].scatter(x, residuals, s=10)
axes[1].axhline(0, linestyle=&#x27;--&#x27;)
axes[1].set_title(&#x27;Residuals&#x27;)

plt.tight_layout()
plt.savefig(&#x27;assets/06-stepwise-regression.png&#x27;, dpi=140)
plt.show()</code></pre>
    </details>
  </section>

  <section class="card">
    <h2>Figure & Interpretation</h2>
    <figure>
      <img src="assets/06-stepwise-regression.png" alt="Demo plot for Stepwise Regression (original data, fitted curve, residuals)" style="width:100%;max-width:100%;" />
      <figcaption>Generated by the example code: Left — original data with fitted curve; Right — residual diagnostics (or probability curve for classifiers).</figcaption>
    </figure>
    <p>The <strong>original data</strong> points are scattered due to added white noise. The <strong>fitted curve</strong> represents model predictions on a dense grid. The <strong>residuals</strong> ($y - \hat y$) highlight model mismatch; patterns may suggest missing features, nonlinearity, or heteroskedasticity.</p>
  </section>

  
    <nav class="chapter-nav bottom">
      <a href="model-05-ridge.html">&larr; Prev</a>
      <a class="index-link" href="index.html">Index</a>
      <a href="model-07-logistic.html">Next &rarr;</a>
    </nav>
    

  <footer class="site-footer">
    <p>&copy; Regression Models Handbook</p>
  </footer>
</main>
</body>
</html>
